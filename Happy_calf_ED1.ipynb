{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calf was last happy on: 2023-09-27 14:40:27\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "current_datetime=datetime.datetime.now()\n",
    "print(f\"Calf was last happy on: {current_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 14:40:28.478461: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-27 14:40:28.536998: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-27 14:40:30.378143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "import glob\n",
    "import absl\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Wed Sep 27 14:40:34 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080        On  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 30%   33C    P8              17W / 300W |      5MiB / 10240MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))\n",
    "!nvidia-smi\n",
    "# !kill -9 1618764\n",
    "gpu_memory_fraction = 0.8\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth for each GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=int(gpu_memory_fraction * 1024))]\n",
    "            )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download EfficientDet model \n",
    "# TODO: \n",
    "# 1. Configure the directory path for the model download.\n",
    "\n",
    "\n",
    "# import wget\n",
    "# model_link = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\"\n",
    "# model_tar_name=\"efficientdet_d0_coco17_tpu-32.tar.gz\"\n",
    "\n",
    "# # Check if model is already downloadeds\n",
    "# if not os.path.exists(model_tar_name):\n",
    "#     wget.download(model_link)\n",
    "#     # Unzip\n",
    "#     !tar -zxvf {model_tar_name} \n",
    "# else:\n",
    "#     print(f\"Model {model_tar_name} already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-27 14:40:35.409508: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-27 14:40:35.456111: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-27 14:40:37.046585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-27 14:40:42.717758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8251 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "I0927 14:40:42.719049 140493556425344 mirrored_strategy.py:419] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: None\n",
      "I0927 14:40:42.868453 140493556425344 config_util.py:552] Maybe overwriting train_steps: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0927 14:40:42.868594 140493556425344 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "I0927 14:40:42.878352 140493556425344 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
      "I0927 14:40:42.878463 140493556425344 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 88\n",
      "I0927 14:40:42.878515 140493556425344 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\n",
      "I0927 14:40:42.882420 140493556425344 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:42.938879 140493556425344 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:42.942160 140493556425344 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:42.947981 140493556425344 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0927 14:40:42.948062 140493556425344 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:42.968425 140493556425344 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:42.971225 140493556425344 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:43.033527 140493556425344 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:43.036289 140493556425344 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:43.060449 140493556425344 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:43.063207 140493556425344 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:43.122302 140493556425344 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:43.125079 140493556425344 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0927 14:40:43.135179 140493556425344 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0927 14:40:43.135258 140493556425344 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0927 14:40:43.461263 140493556425344 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0927 14:40:43.461380 140493556425344 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I0927 14:40:43.743149 140493556425344 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I0927 14:40:43.743251 140493556425344 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I0927 14:40:44.103414 140493556425344 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I0927 14:40:44.103507 140493556425344 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I0927 14:40:44.515470 140493556425344 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I0927 14:40:44.515589 140493556425344 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I0927 14:40:45.039587 140493556425344 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I0927 14:40:45.039690 140493556425344 efficientnet_model.py:143] round_filter input=320 output=320\n",
      "I0927 14:40:45.226576 140493556425344 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
      "I0927 14:40:45.276718 140493556425344 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "WARNING:tensorflow:From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W0927 14:40:45.450555 140493556425344 deprecation.py:364] From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['Calf_Detection/new_app/kalb_train.tfrecord']\n",
      "I0927 14:40:45.470331 140493556425344 dataset_builder.py:162] Reading unweighted datasets: ['Calf_Detection/new_app/kalb_train.tfrecord']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['Calf_Detection/new_app/kalb_train.tfrecord']\n",
      "I0927 14:40:45.470711 140493556425344 dataset_builder.py:79] Reading record datasets for input file: ['Calf_Detection/new_app/kalb_train.tfrecord']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0927 14:40:45.470786 140493556425344 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0927 14:40:45.470838 140493556425344 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W0927 14:40:45.484113 140493556425344 deprecation.py:364] From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0927 14:40:45.504261 140493556425344 deprecation.py:364] From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0927 14:40:50.713418 140493556425344 deprecation.py:364] From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0927 14:40:54.043007 140493556425344 deprecation.py:364] From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "/home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n",
      "I0927 14:41:01.731831 140477307287296 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0927 14:41:09.926934 140477307287296 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2023-09-27 14:41:14.393703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2023-09-27 14:41:14.476754: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-27 14:41:14.836927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "WARNING:tensorflow:From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0927 14:41:20.471591 140477404153600 deprecation.py:569] From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "I0927 14:41:22.564997 140477404153600 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "W0927 14:41:27.417848 140477404153600 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "I0927 14:41:31.762321 140477404153600 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "W0927 14:41:36.731468 140477404153600 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "I0927 14:41:40.168380 140477404153600 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "W0927 14:41:44.823648 140477404153600 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "I0927 14:41:49.227237 140477404153600 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "W0927 14:41:53.802801 140477404153600 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2023-09-27 14:41:59.890161: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inEfficientDet-D1/model/stack_0/block_1/drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-09-27 14:42:15.473166: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "INFO:tensorflow:Step 100 per-step time 0.897s\n",
      "I0927 14:42:49.874684 140493556425344 model_lib_v2.py:705] Step 100 per-step time 0.897s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.7974747,\n",
      " 'Loss/localization_loss': 0.016337866,\n",
      " 'Loss/regularization_loss': 0.029544072,\n",
      " 'Loss/total_loss': 0.8433566,\n",
      " 'learning_rate': 0.00416}\n",
      "I0927 14:42:49.875181 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.7974747,\n",
      " 'Loss/localization_loss': 0.016337866,\n",
      " 'Loss/regularization_loss': 0.029544072,\n",
      " 'Loss/total_loss': 0.8433566,\n",
      " 'learning_rate': 0.00416}\n",
      "INFO:tensorflow:Step 200 per-step time 0.329s\n",
      "I0927 14:43:22.649104 140493556425344 model_lib_v2.py:705] Step 200 per-step time 0.329s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.5379367,\n",
      " 'Loss/localization_loss': 0.02429903,\n",
      " 'Loss/regularization_loss': 0.029603273,\n",
      " 'Loss/total_loss': 0.59183896,\n",
      " 'learning_rate': 0.0073200003}\n",
      "I0927 14:43:22.649578 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.5379367,\n",
      " 'Loss/localization_loss': 0.02429903,\n",
      " 'Loss/regularization_loss': 0.029603273,\n",
      " 'Loss/total_loss': 0.59183896,\n",
      " 'learning_rate': 0.0073200003}\n",
      "INFO:tensorflow:Step 300 per-step time 0.328s\n",
      "I0927 14:43:55.441696 140493556425344 model_lib_v2.py:705] Step 300 per-step time 0.328s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.42878163,\n",
      " 'Loss/localization_loss': 0.025980234,\n",
      " 'Loss/regularization_loss': 0.029680619,\n",
      " 'Loss/total_loss': 0.48444247,\n",
      " 'learning_rate': 0.010480001}\n",
      "I0927 14:43:55.441939 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.42878163,\n",
      " 'Loss/localization_loss': 0.025980234,\n",
      " 'Loss/regularization_loss': 0.029680619,\n",
      " 'Loss/total_loss': 0.48444247,\n",
      " 'learning_rate': 0.010480001}\n",
      "INFO:tensorflow:Step 400 per-step time 0.329s\n",
      "I0927 14:44:28.294656 140493556425344 model_lib_v2.py:705] Step 400 per-step time 0.329s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.51681155,\n",
      " 'Loss/localization_loss': 0.01396137,\n",
      " 'Loss/regularization_loss': 0.029781375,\n",
      " 'Loss/total_loss': 0.5605543,\n",
      " 'learning_rate': 0.0136400005}\n",
      "I0927 14:44:28.295121 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.51681155,\n",
      " 'Loss/localization_loss': 0.01396137,\n",
      " 'Loss/regularization_loss': 0.029781375,\n",
      " 'Loss/total_loss': 0.5605543,\n",
      " 'learning_rate': 0.0136400005}\n",
      "INFO:tensorflow:Step 500 per-step time 0.328s\n",
      "I0927 14:45:01.109345 140493556425344 model_lib_v2.py:705] Step 500 per-step time 0.328s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.56504637,\n",
      " 'Loss/localization_loss': 0.010635432,\n",
      " 'Loss/regularization_loss': 0.029935371,\n",
      " 'Loss/total_loss': 0.60561717,\n",
      " 'learning_rate': 0.016800001}\n",
      "I0927 14:45:01.109655 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.56504637,\n",
      " 'Loss/localization_loss': 0.010635432,\n",
      " 'Loss/regularization_loss': 0.029935371,\n",
      " 'Loss/total_loss': 0.60561717,\n",
      " 'learning_rate': 0.016800001}\n",
      "INFO:tensorflow:Step 600 per-step time 0.328s\n",
      "I0927 14:45:33.948262 140493556425344 model_lib_v2.py:705] Step 600 per-step time 0.328s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.46577665,\n",
      " 'Loss/localization_loss': 0.012553495,\n",
      " 'Loss/regularization_loss': 0.030110857,\n",
      " 'Loss/total_loss': 0.508441,\n",
      " 'learning_rate': 0.019960001}\n",
      "I0927 14:45:33.948516 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.46577665,\n",
      " 'Loss/localization_loss': 0.012553495,\n",
      " 'Loss/regularization_loss': 0.030110857,\n",
      " 'Loss/total_loss': 0.508441,\n",
      " 'learning_rate': 0.019960001}\n",
      "INFO:tensorflow:Step 700 per-step time 0.328s\n",
      "I0927 14:46:06.780482 140493556425344 model_lib_v2.py:705] Step 700 per-step time 0.328s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.6316587,\n",
      " 'Loss/localization_loss': 0.015309817,\n",
      " 'Loss/regularization_loss': 0.03030743,\n",
      " 'Loss/total_loss': 0.6772759,\n",
      " 'learning_rate': 0.023120001}\n",
      "I0927 14:46:06.780926 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.6316587,\n",
      " 'Loss/localization_loss': 0.015309817,\n",
      " 'Loss/regularization_loss': 0.03030743,\n",
      " 'Loss/total_loss': 0.6772759,\n",
      " 'learning_rate': 0.023120001}\n",
      "INFO:tensorflow:Step 800 per-step time 0.328s\n",
      "I0927 14:46:39.623756 140493556425344 model_lib_v2.py:705] Step 800 per-step time 0.328s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.5318936,\n",
      " 'Loss/localization_loss': 0.008041847,\n",
      " 'Loss/regularization_loss': 0.030573156,\n",
      " 'Loss/total_loss': 0.5705086,\n",
      " 'learning_rate': 0.02628}\n",
      "I0927 14:46:39.623997 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.5318936,\n",
      " 'Loss/localization_loss': 0.008041847,\n",
      " 'Loss/regularization_loss': 0.030573156,\n",
      " 'Loss/total_loss': 0.5705086,\n",
      " 'learning_rate': 0.02628}\n",
      "INFO:tensorflow:Step 900 per-step time 0.328s\n",
      "I0927 14:47:12.442015 140493556425344 model_lib_v2.py:705] Step 900 per-step time 0.328s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.8663216,\n",
      " 'Loss/localization_loss': 0.026658881,\n",
      " 'Loss/regularization_loss': 0.030788956,\n",
      " 'Loss/total_loss': 0.9237695,\n",
      " 'learning_rate': 0.02944}\n",
      "I0927 14:47:12.442354 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.8663216,\n",
      " 'Loss/localization_loss': 0.026658881,\n",
      " 'Loss/regularization_loss': 0.030788956,\n",
      " 'Loss/total_loss': 0.9237695,\n",
      " 'learning_rate': 0.02944}\n",
      "INFO:tensorflow:Step 1000 per-step time 0.329s\n",
      "I0927 14:47:45.328501 140493556425344 model_lib_v2.py:705] Step 1000 per-step time 0.329s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 1.1723766,\n",
      " 'Loss/localization_loss': 0.018165857,\n",
      " 'Loss/regularization_loss': 0.031237097,\n",
      " 'Loss/total_loss': 1.2217796,\n",
      " 'learning_rate': 0.0326}\n",
      "I0927 14:47:45.328881 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 1.1723766,\n",
      " 'Loss/localization_loss': 0.018165857,\n",
      " 'Loss/regularization_loss': 0.031237097,\n",
      " 'Loss/total_loss': 1.2217796,\n",
      " 'learning_rate': 0.0326}\n",
      "INFO:tensorflow:Step 1100 per-step time 0.350s\n",
      "I0927 14:48:20.377651 140493556425344 model_lib_v2.py:705] Step 1100 per-step time 0.350s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 1.0179119,\n",
      " 'Loss/localization_loss': 0.015668299,\n",
      " 'Loss/regularization_loss': 0.03177682,\n",
      " 'Loss/total_loss': 1.065357,\n",
      " 'learning_rate': 0.03576}\n",
      "I0927 14:48:20.377914 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 1.0179119,\n",
      " 'Loss/localization_loss': 0.015668299,\n",
      " 'Loss/regularization_loss': 0.03177682,\n",
      " 'Loss/total_loss': 1.065357,\n",
      " 'learning_rate': 0.03576}\n",
      "INFO:tensorflow:Step 1200 per-step time 0.328s\n",
      "I0927 14:48:53.157240 140493556425344 model_lib_v2.py:705] Step 1200 per-step time 0.328s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.5621086,\n",
      " 'Loss/localization_loss': 0.012680762,\n",
      " 'Loss/regularization_loss': 0.032187983,\n",
      " 'Loss/total_loss': 0.60697734,\n",
      " 'learning_rate': 0.03892}\n",
      "I0927 14:48:53.157484 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.5621086,\n",
      " 'Loss/localization_loss': 0.012680762,\n",
      " 'Loss/regularization_loss': 0.032187983,\n",
      " 'Loss/total_loss': 0.60697734,\n",
      " 'learning_rate': 0.03892}\n",
      "INFO:tensorflow:Step 1300 per-step time 0.328s\n",
      "I0927 14:49:25.950758 140493556425344 model_lib_v2.py:705] Step 1300 per-step time 0.328s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.44109717,\n",
      " 'Loss/localization_loss': 0.011058676,\n",
      " 'Loss/regularization_loss': 0.03243598,\n",
      " 'Loss/total_loss': 0.48459184,\n",
      " 'learning_rate': 0.04208}\n",
      "I0927 14:49:25.950998 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.44109717,\n",
      " 'Loss/localization_loss': 0.011058676,\n",
      " 'Loss/regularization_loss': 0.03243598,\n",
      " 'Loss/total_loss': 0.48459184,\n",
      " 'learning_rate': 0.04208}\n",
      "INFO:tensorflow:Step 1400 per-step time 0.327s\n",
      "I0927 14:49:58.687139 140493556425344 model_lib_v2.py:705] Step 1400 per-step time 0.327s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.4918867,\n",
      " 'Loss/localization_loss': 0.00879743,\n",
      " 'Loss/regularization_loss': 0.032678757,\n",
      " 'Loss/total_loss': 0.5333629,\n",
      " 'learning_rate': 0.04524}\n",
      "I0927 14:49:58.687406 140493556425344 model_lib_v2.py:708] {'Loss/classification_loss': 0.4918867,\n",
      " 'Loss/localization_loss': 0.00879743,\n",
      " 'Loss/regularization_loss': 0.032678757,\n",
      " 'Loss/total_loss': 0.5333629,\n",
      " 'learning_rate': 0.04524}\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "!python3 Calf_Detection/models/research/object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path=Calf_Detection/new_app/New_Models/config/efficientdet_d1_coco17_tpu-32.config \\\n",
    "    --model_dir=Calf_Detection/new_app/New_Models/training/epochs_100/efficientdet_d1_coco17_tpu-32 \\\n",
    "    --alsologtostderrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-27 14:50:06.137424: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-27 14:50:06.186759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-27 14:50:09.710202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-27 14:50:18.089693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8251 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "I0927 14:50:18.239592 139652718715520 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
      "I0927 14:50:18.239736 139652718715520 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 88\n",
      "I0927 14:50:18.239785 139652718715520 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\n",
      "I0927 14:50:18.243761 139652718715520 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0927 14:50:18.298448 139652718715520 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I0927 14:50:18.298564 139652718715520 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0927 14:50:18.473351 139652718715520 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I0927 14:50:18.473471 139652718715520 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0927 14:50:18.725380 139652718715520 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I0927 14:50:18.725512 139652718715520 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I0927 14:50:18.995957 139652718715520 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I0927 14:50:18.996076 139652718715520 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I0927 14:50:19.297096 139652718715520 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I0927 14:50:19.297256 139652718715520 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I0927 14:50:19.633949 139652718715520 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I0927 14:50:19.634096 139652718715520 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I0927 14:50:19.990435 139652718715520 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I0927 14:50:19.990569 139652718715520 efficientnet_model.py:143] round_filter input=320 output=320\n",
      "I0927 14:50:20.142887 139652718715520 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
      "I0927 14:50:20.183273 139652718715520 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "WARNING:tensorflow:From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:459: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "W0927 14:50:21.708254 139652718715520 deprecation.py:641] From /home/hpc/iwfa/iwfa030h/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:459: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "I0927 14:50:26.717517 139652718715520 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0927 14:50:36.159115 139652718715520 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "I0927 14:50:39.394794 139652718715520 signature_serialization.py:148] Function `call_func` contains input name(s) resource with unsupported characters which will be renamed to weightsharedconvolutionalboxpredictor_classpredictiontower_conv2d_2_batchnorm_feature_4_fusedbatchnormv3_readvariableop_1_resource in the SavedModel.\n",
      "I0927 14:50:41.984273 139652718715520 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f023c00da30>, because it is not built.\n",
      "W0927 14:50:43.813595 139652718715520 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f023c00da30>, because it is not built.\n",
      "I0927 14:51:11.384543 139652718715520 save.py:274] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 535). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: Calf_Detection/new_app/New_Models/finetuned/epochs_100/efficientdet_d1_coco17_tpu-32/saved_model/assets\n",
      "I0927 14:51:32.940455 139652718715520 builder_impl.py:804] Assets written to: Calf_Detection/new_app/New_Models/finetuned/epochs_100/efficientdet_d1_coco17_tpu-32/saved_model/assets\n",
      "I0927 14:51:33.818918 139652718715520 fingerprinting_utils.py:48] Writing fingerprint to Calf_Detection/new_app/New_Models/finetuned/epochs_100/efficientdet_d1_coco17_tpu-32/saved_model/fingerprint.pb\n",
      "INFO:tensorflow:Writing pipeline config file to Calf_Detection/new_app/New_Models/finetuned/epochs_100/efficientdet_d1_coco17_tpu-32/pipeline.config\n",
      "I0927 14:51:35.618699 139652718715520 config_util.py:253] Writing pipeline config file to Calf_Detection/new_app/New_Models/finetuned/epochs_100/efficientdet_d1_coco17_tpu-32/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "# Export the model d0\n",
    "pipeline_file = 'Calf_Detection/new_app/New_Models/config/efficientdet_d1_coco17_tpu-32.config'\n",
    "last_model_path = 'Calf_Detection/new_app/New_Models/training/epochs_100/efficientdet_d1_coco17_tpu-32'\n",
    "output_directory = \"Calf_Detection/new_app/New_Models/finetuned/epochs_100/efficientdet_d1_coco17_tpu-32\"\n",
    "!python3 Calf_Detection/models/research/object_detection/exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir {last_model_path} \\\n",
    "    --output_directory {output_directory} \\\n",
    "    --pipeline_config_path {pipeline_file}\n",
    "\n",
    "# # Export the model d1\n",
    "# pipeline_file = 'new_app/New_Models/config/efficientdet_d1_coco17_tpu-32.config'\n",
    "# last_model_path = 'new_app/New_Models/training/efficientdet_d1_coco17_tpu-32'\n",
    "# output_directory = \"new_app/New_Models/finetuned/efficientdet_d1_coco17_tpu-32\"\n",
    "# !python3 models/research/object_detection/exporter_main_v2.py \\\n",
    "#     --trained_checkpoint_dir {last_model_path} \\\n",
    "#     --output_directory {output_directory} \\\n",
    "#     --pipeline_config_path {pipeline_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 14:51:38.275737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 819 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:1a:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f040c4578b0>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received incompatible tensor with shape (1, 1, 1152, 48) when attempting to restore variable with shape (1, 1, 480, 112) and name stack_4/block_0/project_conv2d/kernel:0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:758\u001b[0m, in \u001b[0;36mBaseResourceVariable._restore_from_tensors\u001b[0;34m(self, restored_tensors)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m   assigned_variable \u001b[39m=\u001b[39m shape_safe_assign_variable_handle(\n\u001b[1;32m    759\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, restored_tensor)\n\u001b[1;32m    760\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:313\u001b[0m, in \u001b[0;36mshape_safe_assign_variable_handle\u001b[0;34m(handle, shape, value, name)\u001b[0m\n\u001b[1;32m    312\u001b[0m   value_tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(value)\n\u001b[0;32m--> 313\u001b[0m shape\u001b[39m.\u001b[39;49massert_is_compatible_with(value_tensor\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m gen_resource_variable_ops\u001b[39m.\u001b[39massign_variable_op(\n\u001b[1;32m    315\u001b[0m     handle, value_tensor, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/framework/tensor_shape.py:1361\u001b[0m, in \u001b[0;36mTensorShape.assert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_compatible_with(other):\n\u001b[0;32m-> 1361\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mShapes \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m are incompatible\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m, other))\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (1, 1, 480, 112) and (1, 1, 1152, 48) are incompatible",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/woody/iwfa/iwfa030h/Calf_Detection/Happy_calf_ED1.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/woody/iwfa/iwfa030h/Calf_Detection/Happy_calf_ED1.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Restore checkpoint\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/woody/iwfa/iwfa030h/Calf_Detection/Happy_calf_ED1.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m ckpt \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv2\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mCheckpoint(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/woody/iwfa/iwfa030h/Calf_Detection/Happy_calf_ED1.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m       model\u001b[39m=\u001b[39mdetection_model)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/woody/iwfa/iwfa030h/Calf_Detection/Happy_calf_ED1.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m ckpt\u001b[39m.\u001b[39;49mrestore(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mstr\u001b[39;49m(filenames[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m.index\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)))\u001b[39m.\u001b[39mexpect_partial()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/woody/iwfa/iwfa030h/Calf_Detection/Happy_calf_ED1.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mprint\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(filenames[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.index\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btinyx.nhr.fau.de/home/woody/iwfa/iwfa030h/Calf_Detection/Happy_calf_ED1.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_model_detection_function\u001b[39m(model):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:2677\u001b[0m, in \u001b[0;36mCheckpoint.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2674\u001b[0m   save_path \u001b[39m=\u001b[39m path_helpers\u001b[39m.\u001b[39mget_variables_path(save_path)\n\u001b[1;32m   2676\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2677\u001b[0m   status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(save_path, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m   2678\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   2679\u001b[0m     context\u001b[39m.\u001b[39masync_wait()  \u001b[39m# Ensure restore operations have completed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:2540\u001b[0m, in \u001b[0;36mCheckpoint.read\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2538\u001b[0m   save_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(save_path)\n\u001b[1;32m   2539\u001b[0m options \u001b[39m=\u001b[39m options \u001b[39mor\u001b[39;00m checkpoint_options\u001b[39m.\u001b[39mCheckpointOptions()\n\u001b[0;32m-> 2540\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_saver\u001b[39m.\u001b[39;49mrestore(save_path\u001b[39m=\u001b[39;49msave_path, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m   2541\u001b[0m metrics\u001b[39m.\u001b[39mAddCheckpointReadDuration(\n\u001b[1;32m   2542\u001b[0m     api_label\u001b[39m=\u001b[39m_CHECKPOINT_V2,\n\u001b[1;32m   2543\u001b[0m     microseconds\u001b[39m=\u001b[39m_get_duration_microseconds(start_time, time\u001b[39m.\u001b[39mtime()))\n\u001b[1;32m   2544\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:1461\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1451\u001b[0m object_graph_proto\u001b[39m.\u001b[39mParseFromString(object_graph_string)\n\u001b[1;32m   1452\u001b[0m checkpoint \u001b[39m=\u001b[39m _CheckpointRestoreCoordinator(\n\u001b[1;32m   1453\u001b[0m     object_graph_proto\u001b[39m=\u001b[39mobject_graph_proto,\n\u001b[1;32m   1454\u001b[0m     save_path\u001b[39m=\u001b[39msave_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   1460\u001b[0m     saveables_cache\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_saveables_cache)\n\u001b[0;32m-> 1461\u001b[0m restore_lib\u001b[39m.\u001b[39;49mCheckpointPosition(\n\u001b[1;32m   1462\u001b[0m     checkpoint\u001b[39m=\u001b[39;49mcheckpoint, proto_id\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mrestore(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph_view\u001b[39m.\u001b[39;49mroot,\n\u001b[1;32m   1463\u001b[0m                                                reader)\n\u001b[1;32m   1465\u001b[0m \u001b[39m# Attached dependencies are not attached to the root, so should be restored\u001b[39;00m\n\u001b[1;32m   1466\u001b[0m \u001b[39m# separately.\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_view\u001b[39m.\u001b[39mattached_dependencies:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/checkpoint/restore.py:62\u001b[0m, in \u001b[0;36mCheckpointPosition.restore\u001b[0;34m(self, trackable, reader)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39minit_scope():\n\u001b[1;32m     59\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbind_object(trackable):\n\u001b[1;32m     60\u001b[0m     \u001b[39m# This object's correspondence with a checkpointed object is new, so\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[39m# process deferred restorations for it and its dependencies.\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     restore_ops \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_restore_descendants(reader)\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m restore_ops:\n\u001b[1;32m     64\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint\u001b[39m.\u001b[39mnew_restore_ops(restore_ops)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/checkpoint/restore.py:463\u001b[0m, in \u001b[0;36mCheckpointPosition._restore_descendants\u001b[0;34m(self, reader)\u001b[0m\n\u001b[1;32m    459\u001b[0m   _queue_children_for_restoration(current_position, visit_queue)\n\u001b[1;32m    460\u001b[0m   _queue_slot_variables(current_position, visit_queue)\n\u001b[1;32m    462\u001b[0m restore_ops\u001b[39m.\u001b[39mextend(\n\u001b[0;32m--> 463\u001b[0m     current_position\u001b[39m.\u001b[39;49mcheckpoint\u001b[39m.\u001b[39;49mrestore_saveables(\n\u001b[1;32m    464\u001b[0m         tensor_saveables,\n\u001b[1;32m    465\u001b[0m         python_positions,\n\u001b[1;32m    466\u001b[0m         registered_savers,\n\u001b[1;32m    467\u001b[0m         reader\u001b[39m=\u001b[39;49mreader))\n\u001b[1;32m    468\u001b[0m \u001b[39mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/checkpoint/checkpoint.py:360\u001b[0m, in \u001b[0;36m_CheckpointRestoreCoordinator.restore_saveables\u001b[0;34m(self, tensor_saveables, python_positions, registered_savers, reader)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m tensor_saveables \u001b[39mor\u001b[39;00m registered_savers:\n\u001b[1;32m    358\u001b[0m   flat_saveables \u001b[39m=\u001b[39m saveable_object_util\u001b[39m.\u001b[39mvalidate_and_slice_inputs(\n\u001b[1;32m    359\u001b[0m       tensor_saveables)\n\u001b[0;32m--> 360\u001b[0m   new_restore_ops \u001b[39m=\u001b[39m functional_saver\u001b[39m.\u001b[39;49mMultiDeviceSaver\u001b[39m.\u001b[39;49mfrom_saveables(\n\u001b[1;32m    361\u001b[0m       flat_saveables,\n\u001b[1;32m    362\u001b[0m       registered_savers)\u001b[39m.\u001b[39;49mrestore(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_path_tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m    363\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    364\u001b[0m     \u001b[39mfor\u001b[39;00m name, restore_op \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(new_restore_ops\u001b[39m.\u001b[39mitems()):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/checkpoint/functional_saver.py:499\u001b[0m, in \u001b[0;36mMultiDeviceSaver.restore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    497\u001b[0m   restore_ops \u001b[39m=\u001b[39m tf_function_restore()\n\u001b[1;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m   restore_ops \u001b[39m=\u001b[39m restore_fn()\n\u001b[1;32m    501\u001b[0m \u001b[39mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/checkpoint/functional_saver.py:467\u001b[0m, in \u001b[0;36mMultiDeviceSaver.restore.<locals>.restore_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mfor\u001b[39;00m ckpt_key, tensor \u001b[39min\u001b[39;00m restore_fn_inputs[restore_fn]\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    465\u001b[0m   restored_tensors[trackable_utils\u001b[39m.\u001b[39mextract_local_name(\n\u001b[1;32m    466\u001b[0m       ckpt_key)] \u001b[39m=\u001b[39m tensor\n\u001b[0;32m--> 467\u001b[0m ret \u001b[39m=\u001b[39m restore_fn(restored_tensors)\n\u001b[1;32m    468\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    469\u001b[0m   restore_ops\u001b[39m.\u001b[39mupdate(ret)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/training/saving/saveable_object_util.py:734\u001b[0m, in \u001b[0;36mSaveableCompatibilityConverter._restore_from_tensors\u001b[0;34m(self, restored_tensors)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(expected_keys) \u001b[39m!=\u001b[39m restored_tensors\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    729\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not restore object \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj\u001b[39m}\u001b[39;00m\u001b[39m because not all \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mexpected tensors were in the checkpoint.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mGot: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(restored_tensors\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 734\u001b[0m \u001b[39mreturn\u001b[39;00m saveable_object_to_restore_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msaveables)(restored_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/training/saving/saveable_object_util.py:771\u001b[0m, in \u001b[0;36msaveable_object_to_restore_fn.<locals>._restore_from_tensors\u001b[0;34m(restored_tensors)\u001b[0m\n\u001b[1;32m    768\u001b[0m       maybe_tensor \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m: maybe_tensor}\n\u001b[1;32m    770\u001b[0m     saveable_restored_tensors\u001b[39m.\u001b[39mappend(maybe_tensor[slice_spec])\n\u001b[0;32m--> 771\u001b[0m   restore_ops[saveable\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m saveable\u001b[39m.\u001b[39;49mrestore(\n\u001b[1;32m    772\u001b[0m       saveable_restored_tensors, restored_shapes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    773\u001b[0m \u001b[39mreturn\u001b[39;00m restore_ops\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/training/saving/saveable_object_util.py:600\u001b[0m, in \u001b[0;36mTrackableSaveable.restore\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    598\u001b[0m   ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_mapped_captures(restore_fn, [restored_tensor_dict])\n\u001b[1;32m    599\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m   ret \u001b[39m=\u001b[39m restore_fn(restored_tensor_dict)\n\u001b[1;32m    601\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m   \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_od/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:761\u001b[0m, in \u001b[0;36mBaseResourceVariable._restore_from_tensors\u001b[0;34m(self, restored_tensors)\u001b[0m\n\u001b[1;32m    758\u001b[0m   assigned_variable \u001b[39m=\u001b[39m shape_safe_assign_variable_handle(\n\u001b[1;32m    759\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, restored_tensor)\n\u001b[1;32m    760\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 761\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    762\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived incompatible tensor with shape \u001b[39m\u001b[39m{\u001b[39;00mrestored_tensor\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhen attempting to restore variable with shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    764\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand name \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[39mreturn\u001b[39;00m assigned_variable\n",
      "\u001b[0;31mValueError\u001b[0m: Received incompatible tensor with shape (1, 1, 1152, 48) when attempting to restore variable with shape (1, 1, 480, 112) and name stack_4/block_0/project_conv2d/kernel:0."
     ]
    }
   ],
   "source": [
    "pipeline_file = 'Calf_Detection/new_app/New_Models/config/efficientdet_d1_coco17_tpu-32.config'\n",
    "last_model_path = 'Calf_Detection/new_app/New_Models/training/epochs_100/efficientdet_d1_coco17_tpu-32'\n",
    "output_directory = \"Calf_Detection/new_app/New_Models/finetuned/epochs_100/efficientdet_d1_coco17_tpu-32\"\n",
    "\n",
    "import pathlib\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Load an image from file into a numpy array.\n",
    "  Puts image into numpy array to feed into tensorflow graph.\n",
    "  Note that by convention we put it into a numpy array with shape\n",
    "  (height, width, channels), where channels=3 for RGB.\n",
    "  Args:\n",
    "    path: a file path.\n",
    "  Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "  image = Image.open(BytesIO(img_data))\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def plot_detections(image_np,\n",
    "                    boxes,\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    category_index,\n",
    "                    figsize=(12, 16),\n",
    "                    image_name=None):\n",
    "  image_np_with_annotations = image_np.copy()\n",
    "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_annotations,\n",
    "      boxes,\n",
    "      classes,\n",
    "      scores,\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      min_score_thresh=0.8)\n",
    "  if image_name:\n",
    "    plt.imsave(image_name, image_np_with_annotations)\n",
    "  else:\n",
    "    plt.imshow(image_np_with_annotations)\n",
    "\n",
    "filenames = list(pathlib.Path('Calf_Detection/new_app/training_ed0_epochs_100').glob('*.index'))\n",
    "filenames.sort()\n",
    "\n",
    "# recover our saved model\n",
    "model_dir = 'Calf_Detection/new_app/training_ed0_epochs_100'\n",
    "# Adding the last checkpoint\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_file)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=False)\n",
    "\n",
    "print(detection_model)\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(\n",
    "      model=detection_model)\n",
    "ckpt.restore(os.path.join(str(filenames[-1]).replace('.index',''))).expect_partial()\n",
    "print(os.path.join(str(filenames[-1]).replace('.index','')))\n",
    "\n",
    "def get_model_detection_function(model):\n",
    "  \"\"\"Get a tf.function for detection.\"\"\"\n",
    "\n",
    "  @tf.function\n",
    "  def detect_fn(image):\n",
    "    \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "    image, shapes = model.preprocess(image)\n",
    "    prediction_dict = model.predict(image, shapes)\n",
    "    detections = model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "\n",
    "  return detect_fn\n",
    "\n",
    "detect_fn = get_model_detection_function(detection_model)\n",
    "# map labels for inference decoding\n",
    "label_map_path = configs['eval_input_config'].label_map_path\n",
    "label_map = label_map_util.load_labelmap(label_map_path)\n",
    "categories = label_map_util.convert_label_map_to_categories(\n",
    "    label_map,\n",
    "    max_num_classes=label_map_util.get_max_label_map_index(label_map),\n",
    "    use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)\n",
    "#run detector on test image\n",
    "#it takes a little longer on the first run and then runs at normal speed. \n",
    "\n",
    "# TEST_IMAGE_PATHS = glob.glob('Calf_Detection/new_app/test/2019-06-29_090001.jpg')\n",
    "TEST_IMAGE_PATHS = glob.glob('Calf_Detection/new_app/test/2018-11-12_031001.jpg')\n",
    "\n",
    "image_path = random.choice(TEST_IMAGE_PATHS)\n",
    "image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(\n",
    "    np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "\n",
    "label_id_offset = 1\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_detections,\n",
    "      detections['detection_boxes'][0].numpy(),\n",
    "      (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "      detections['detection_scores'][0].numpy(),\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=1,\n",
    "      min_score_thresh=.1,\n",
    "      agnostic_mode=False,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,16))\n",
    "plt.imshow(image_np_with_detections)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# sample_image_path = 'Calf_Detection/new_app/test/2019-06-29_090001.jpg'\n",
    "# sample_image_np = load_image_into_numpy_array(sample_image_path)\n",
    "\n",
    "# # Define the number of inference iterations\n",
    "\n",
    "# # Assuming 24 fps x 60 seconds = 1440 frames per minute\n",
    "# num_iterations = 1440\n",
    "\n",
    "# total_inference_time = 0\n",
    "\n",
    "# for _ in range(num_iterations):\n",
    "#     input_tensor = tf.convert_to_tensor(\n",
    "#         np.expand_dims(sample_image_np, 0), dtype=tf.float32)\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "#     end_time = time.time()\n",
    "    \n",
    "#     inference_time = end_time - start_time\n",
    "#     total_inference_time += inference_time\n",
    "\n",
    "# average_inference_time = total_inference_time \n",
    "# print(f\"Total Inference Time for 1 Minute: {average_inference_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "# !python3 Calf_Detection/models/research/object_detection/model_main_tf2.py \\\n",
    "#     --pipeline_config_path=Calf_Detection/new_app/config/efficientdet_d0_coco17_tpu-32.config \\\n",
    "#     --model_dir=Calf_Detection/new_app/training_ed0 \\\n",
    "#     --checkpoint_dir=Calf_Detection/new_app/training_ed0 \\\n",
    "#     --alsologtostderr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 14:54:26.728462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2023-09-27 14:54:26.809339: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-27 14:54:27.174736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.00\n",
      "Recall: 0.31\n",
      "Accuracy: 0.31\n",
      "\n",
      "Confusion Matrix:\n",
      "TP: 139\n",
      "FP: 0\n",
      "TN: 0\n",
      "FN: 316\n",
      "\n",
      "Images that didn't meet the threshold:\n",
      "Calf_Detection/new_app/test/2019-01-13_235001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_111001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-03_040501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_172501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_155001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_221501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_092501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_170501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_155001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_043001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_165501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_233501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_234001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-16_033501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-16_010501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_112501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_220001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_212501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_005501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_211001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_210001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_012501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-01_004001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_011001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-29_111501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_050501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_075001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_131501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_103001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_211501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_225501.jpg\n",
      "Calf_Detection/new_app/test/2019-05-21_110001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_064501.jpg\n",
      "Calf_Detection/new_app/test/2019-07-12_024501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_082001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_102501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_210501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_173001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_090001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_232001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_094501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-30_100001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_110001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_071001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_030501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-24_070001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_063001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_155502.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_225501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_230002.jpg\n",
      "Calf_Detection/new_app/test/2019-02-28_200001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_095001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-03_080501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_093501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_220501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-19_123001.jpg\n",
      "Calf_Detection/new_app/test/2019-02-16_050001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-19_123501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_115501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-05_075502.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_233501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_060001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-29_073001.jpg\n",
      "Calf_Detection/new_app/test/2019-07-08_065501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_041501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_112501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-16_020501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_052501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_034001.jpg\n",
      "Calf_Detection/new_app/test/2019-02-28_212501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_134001.jpg\n",
      "Calf_Detection/new_app/test/2019-07-12_223001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_225501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-11_014501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_222001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_134001.jpg\n",
      "Calf_Detection/new_app/test/2019-07-18_222501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_080501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_073001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_230001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-25_024501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_231501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_163501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_140501.jpg\n",
      "Calf_Detection/new_app/test/2019-02-28_220001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_110501.jpg\n",
      "Calf_Detection/new_app/test/2019-03-22_115501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_181001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_041501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_165501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_111501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_095501.jpg\n",
      "Calf_Detection/new_app/test/2019-08-15_173501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_015001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_231501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-03_001001.jpg\n",
      "Calf_Detection/new_app/test/2019-04-30_093001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_212001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_063001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-19_105501.jpg\n",
      "Calf_Detection/new_app/test/2019-02-25_002501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_201001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_102501.jpg\n",
      "Calf_Detection/new_app/test/2019-03-03_121001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_160001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_085001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_055501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_051501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_213501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_062001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_070001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_062501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-29_024001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_054501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_090501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_230001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_035501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_062001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_125001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_094001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-16_043501.jpg\n",
      "Calf_Detection/new_app/test/2019-05-21_115501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_161501.jpg\n",
      "Calf_Detection/new_app/test/2019-09-08_214501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_044001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_200501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_024501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_093001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-03_021001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_185001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_044501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_022501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_011501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_030001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_135001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_042501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_085001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-29_091501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_010001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_205501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_090001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_031501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_201501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_221001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_010001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_114501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_023001.jpg\n",
      "Calf_Detection/new_app/test/2019-03-22_111001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_052001.jpg\n",
      "Calf_Detection/new_app/test/2019-03-31_221001.jpg\n",
      "Calf_Detection/new_app/test/2019-02-06_144001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_154501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_232501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_031001.jpg\n",
      "Calf_Detection/new_app/test/2019-03-31_000501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_072001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_014001.jpg\n",
      "Calf_Detection/new_app/test/2019-03-04_023501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-29_025001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_114501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-16_102001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_234501.jpg\n",
      "Calf_Detection/new_app/test/2019-02-08_115001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-19_121001.jpg\n",
      "Calf_Detection/new_app/test/2019-04-11_063001.jpg\n",
      "Calf_Detection/new_app/test/2019-03-31_005001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_222501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_132501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_040001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-19_125001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_205001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_050001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_084501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_132501.jpg\n",
      "Calf_Detection/new_app/test/2019-03-31_211501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_113001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_100001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_211001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_074001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_234001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_113001.jpg\n",
      "Calf_Detection/new_app/test/2019-03-23_224501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-16_011501.jpg\n",
      "Calf_Detection/new_app/test/2019-07-12_192001.jpg\n",
      "Calf_Detection/new_app/test/2019-07-13_195001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_055001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_065001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_115001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_110001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-16_210001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_114001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_032001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_063501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_081501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_031501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_041001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-25_001501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_062501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_040501.jpg\n",
      "Calf_Detection/new_app/test/2019-03-02_145001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_065501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-25_124001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_074501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_064001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_183001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_010501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_075501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-29_043501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_093501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_175501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_142501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-30_083001.jpg\n",
      "Calf_Detection/new_app/test/2019-02-07_055001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-29_021501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-20_053001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_040002.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_155501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_053001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-16_100001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-29_070501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-01_003001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_043001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_073501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_044501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_080001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_082501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_101001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_010501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_164501.jpg\n",
      "Calf_Detection/new_app/test/2019-07-12_085001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-03_051501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_133001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_145501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_160501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_232501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-18_150001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_090501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_053001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_044001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_062501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-25_000001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_203501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_153501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-09_044001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-29_030001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_053001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_080501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_011001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-16_014001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_030501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_162001.jpg\n",
      "Calf_Detection/new_app/test/2019-06-26_074001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_201001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_153001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_224501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_070501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_235501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_102501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_111001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_015501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_034501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_100001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-16_032502.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_065001.jpg\n",
      "Calf_Detection/new_app/test/2019-02-23_205001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_093001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_131001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_151501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_211501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_011001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_180001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_113501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-03_023501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-02_153001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-03_075501.jpg\n",
      "Calf_Detection/new_app/test/2019-06-30_164501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-18_011001.jpg\n",
      "Calf_Detection/new_app/test/2019-02-09_020001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_043501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-20_002501.jpg\n",
      "Calf_Detection/new_app/test/2019-07-03_195001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_072501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_191001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_095001.jpg\n",
      "Calf_Detection/new_app/test/2019-02-06_085001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_071001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_101501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_222501.jpg\n",
      "Calf_Detection/new_app/test/2019-04-10_224501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_005501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-24_071001.jpg\n",
      "Calf_Detection/new_app/test/2019-06-02_230001.jpg\n",
      "Calf_Detection/new_app/test/2019-06-08_020501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-23_113002.jpg\n",
      "Calf_Detection/new_app/test/2019-01-18_151501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-20_061001.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_175001.jpg\n",
      "Calf_Detection/new_app/test/2019-07-12_041001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_142001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_080501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-14_063001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_013501.jpg\n",
      "Calf_Detection/new_app/test/2019-05-19_130501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-19_070501.jpg\n",
      "Calf_Detection/new_app/test/2018-12-24_190001.jpg\n",
      "Calf_Detection/new_app/test/2018-11-12_152001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_183501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_220001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-03_052001.jpg\n",
      "Calf_Detection/new_app/test/2019-07-08_045501.jpg\n",
      "Calf_Detection/new_app/test/2018-11-29_092001.jpg\n",
      "Calf_Detection/new_app/test/2019-03-07_035001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-13_233001.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_215501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-15_225501.jpg\n",
      "Calf_Detection/new_app/test/2019-01-04_051001.jpg\n",
      "316\n"
     ]
    }
   ],
   "source": [
    "# Metric Evaluation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "\n",
    "# Paths setup\n",
    "TEST_IMAGE_PATHS = glob.glob('Calf_Detection/new_app/test/*.jpg')\n",
    "\n",
    "# Metrics Initialization\n",
    "false_negative = 0\n",
    "true_negative = 0\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "total_ground_truth = len(TEST_IMAGE_PATHS)\n",
    "low_iou_images = []\n",
    "pipeline_file = 'Calf_Detection/new_app/New_Models/config/efficientdet_d1_coco17_tpu-32.config'\n",
    "last_model_path = 'Calf_Detection/new_app/New_Models/training/epochs_100/efficientdet_d1_coco17_tpu-32'\n",
    "output_directory = \"Calf_Detection/new_app/New_Models/finetuned/epochs_100/efficientdet_d1_coco17_tpu-32\"\n",
    "filenames = list(pathlib.Path('Calf_Detection/new_app/New_Models/training/epochs_100/efficientdet_d1_coco17_tpu-32').glob('*.index'))\n",
    "filenames.sort()\n",
    "model_dir = 'Calf_Detection/new_app/New_Models/training/epochs_100/efficientdet_d1_coco17_tpu-32'\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "  image = Image.open(BytesIO(img_data))\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"Computes Intersection over Union (IoU) between two bounding boxes.\"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    \n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    \n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def extract_bbox_from_xml(xml_path):\n",
    "    \"\"\"Extracts bounding box information from an XML annotation file.\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    for member in root.findall('object'):\n",
    "        bbox = member.find('bndbox')\n",
    "        return [int(bbox.find(pos).text) for pos in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "\n",
    "def get_ground_truth_for_image(image_path):\n",
    "    \"\"\"Gets ground truth bounding box for a given image.\"\"\"\n",
    "    xml_path = image_path.replace('.jpg', '.xml')  \n",
    "    return extract_bbox_from_xml(xml_path)\n",
    "\n",
    "def load_model(pipeline_file, model_dir):\n",
    "    \"\"\"Loads the saved model from checkpoint.\"\"\"\n",
    "    configs = config_util.get_configs_from_pipeline_file(pipeline_file)\n",
    "    model_config = configs['model']\n",
    "    detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "    \n",
    "    # Restore checkpoint\n",
    "    filenames = list(pathlib.Path(model_dir).glob('*.index'))\n",
    "    filenames.sort()\n",
    "    checkpoint_path = str(filenames[-1]).replace('.index','')\n",
    "    ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "    ckpt.restore(checkpoint_path).expect_partial()\n",
    "    \n",
    "    return detection_model\n",
    "\n",
    "def detect_objects(detection_model, image_np):\n",
    "    \"\"\"Detects objects in an image using the trained model.\"\"\"\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor, detection_model)\n",
    "    return detections\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image, model):\n",
    "    \"\"\"TF function for object detection.\"\"\"\n",
    "    image, shapes = model.preprocess(image)\n",
    "    prediction_dict = model.predict(image, shapes)\n",
    "    detections = model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "# Loading model\n",
    "detection_model = load_model(pipeline_file, model_dir)\n",
    "thresholds = np.linspace(0, 1, 50)  # For example, 50 thresholds from 0 to 1\n",
    "tpr_list = []  # Store true positive rates for each threshold\n",
    "fpr_list = []  # Store false positive rates for each threshold\n",
    "\n",
    "\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    image_np = load_image_into_numpy_array(image_path)\n",
    "    detections = detect_objects(detection_model, image_np)\n",
    "    \n",
    "    detection_boxes = detections['detection_boxes'][0].numpy()\n",
    "    detection_scores = detections['detection_scores'][0].numpy()\n",
    "    valid_indices = np.where(detection_scores >= 0.5)[0]\n",
    "    valid_boxes = detection_boxes[valid_indices]\n",
    "\n",
    "    ground_truth_box = get_ground_truth_for_image(image_path)\n",
    "\n",
    "    # If no ground truth for this image, assume it means no calf present\n",
    "    if not ground_truth_box:\n",
    "        if len(valid_boxes) == 0:  # No detections\n",
    "            true_negative += 1\n",
    "        else:\n",
    "            false_positive += 1  # Model detected something when it shouldn't have\n",
    "        continue  # Move to the next image\n",
    "\n",
    "    # Normalize the ground truth bounding box\n",
    "    img_height, img_width, _ = image_np.shape\n",
    "    normalized_gt_box = [\n",
    "        ground_truth_box[1] / img_width,   # xmin\n",
    "        ground_truth_box[0] / img_height,  # ymin\n",
    "        ground_truth_box[3] / img_width,   # xmax\n",
    "        ground_truth_box[2] / img_height   # ymax\n",
    "    ]\n",
    "\n",
    "    detected = any(compute_iou(box, normalized_gt_box) >= 0.50 for box in valid_boxes)\n",
    "\n",
    "    if detected:\n",
    "        true_positive += 1\n",
    "    else:\n",
    "        false_negative += 1  # Model didn't detect when it should have\n",
    "        low_iou_images.append(image_path)\n",
    "\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / total_ground_truth\n",
    "accuracy = (true_positive + true_negative) / total_ground_truth\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"TP: {true_positive}\")\n",
    "print(f\"FP: {false_positive}\")\n",
    "print(f\"TN: {true_negative}\")\n",
    "print(f\"FN: {false_negative}\")\n",
    "\n",
    "print(\"\\nImages that didn't meet the threshold:\")\n",
    "for img in low_iou_images:\n",
    "    print(img)\n",
    "\n",
    "print(len(low_iou_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metric Evaluation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "\n",
    "# Paths setup\n",
    "TEST_IMAGE_PATHS = glob.glob('Calf_Detection/new_app/test/*.jpg')\n",
    "\n",
    "# Metrics Initialization\n",
    "false_negative = 0\n",
    "true_negative = 0\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "total_ground_truth = len(TEST_IMAGE_PATHS)\n",
    "low_iou_images = []\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"Computes Intersection over Union (IoU) between two bounding boxes.\"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    \n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    \n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def extract_bbox_from_xml(xml_path):\n",
    "    \"\"\"Extracts bounding box information from an XML annotation file.\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    for member in root.findall('object'):\n",
    "        bbox = member.find('bndbox')\n",
    "        return [int(bbox.find(pos).text) for pos in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "\n",
    "def get_ground_truth_for_image(image_path):\n",
    "    \"\"\"Gets ground truth bounding box for a given image.\"\"\"\n",
    "    xml_path = image_path.replace('.jpg', '.xml')  \n",
    "    return extract_bbox_from_xml(xml_path)\n",
    "\n",
    "def load_model(pipeline_file, model_dir):\n",
    "    \"\"\"Loads the saved model from checkpoint.\"\"\"\n",
    "    configs = config_util.get_configs_from_pipeline_file(pipeline_file)\n",
    "    model_config = configs['model']\n",
    "    detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "    \n",
    "    # Restore checkpoint\n",
    "    filenames = list(pathlib.Path(model_dir).glob('*.index'))\n",
    "    filenames.sort()\n",
    "    checkpoint_path = str(filenames[-1]).replace('.index','')\n",
    "    ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "    ckpt.restore(checkpoint_path).expect_partial()\n",
    "    \n",
    "    return detection_model\n",
    "\n",
    "def detect_objects(detection_model, image_np):\n",
    "    \"\"\"Detects objects in an image using the trained model.\"\"\"\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor, detection_model)\n",
    "    return detections\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image, model):\n",
    "    \"\"\"TF function for object detection.\"\"\"\n",
    "    image, shapes = model.preprocess(image)\n",
    "    prediction_dict = model.predict(image, shapes)\n",
    "    detections = model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "thresholds = [0.25, 0.50, 0.75, 1.00]\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Reset the metrics for each threshold\n",
    "    false_negative = 0\n",
    "    true_negative = 0\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    \n",
    "    for image_path in TEST_IMAGE_PATHS:\n",
    "        image_np = load_image_into_numpy_array(image_path)\n",
    "        detections = detect_objects(detection_model, image_np)\n",
    "        \n",
    "        detection_boxes = detections['detection_boxes'][0].numpy()\n",
    "        detection_scores = detections['detection_scores'][0].numpy()\n",
    "        valid_indices = np.where(detection_scores >= threshold)[0]\n",
    "        valid_boxes = detection_boxes[valid_indices]\n",
    "\n",
    "        ground_truth_box = get_ground_truth_for_image(image_path)\n",
    "\n",
    "        if not ground_truth_box:\n",
    "            if len(valid_boxes) == 0:\n",
    "                true_negative += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "            continue\n",
    "\n",
    "        img_height, img_width, _ = image_np.shape\n",
    "        normalized_gt_box = [\n",
    "            ground_truth_box[1] / img_width,\n",
    "            ground_truth_box[0] / img_height,\n",
    "            ground_truth_box[3] / img_width,\n",
    "            ground_truth_box[2] / img_height\n",
    "        ]\n",
    "        detected = any(compute_iou(box, normalized_gt_box) >= 0.50 for box in valid_boxes)\n",
    "\n",
    "        if detected:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "    \n",
    "    tpr = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0.0\n",
    "    if (false_positive + true_negative) == 0:\n",
    "        fpr = 0.0\n",
    "    else:\n",
    "        fpr = false_positive / (false_positive + true_negative)\n",
    "    \n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_list, tpr_list, '-o', label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# Annotate each point with its threshold value\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    plt.annotate(f\"{threshold:.2f}\", (fpr_list[i], tpr_list[i]), \n",
    "                 textcoords=\"offset points\", \n",
    "                 xytext=(0,10), \n",
    "                 ha='center')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "save_path = 'Calf_Detection/new_app/New_Models/graphs/roc_curve_d1.png'\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Copy the images from low_iou_images to a new directory\n",
    "import shutil\n",
    "NEW_DIR = \"Calf_Detection/new_app/low_iou_images_dir\"\n",
    "if not os.path.exists(NEW_DIR):\n",
    "    os.makedirs(NEW_DIR)\n",
    "\n",
    "for image_path in low_iou_images:\n",
    "    shutil.copy(image_path, NEW_DIR)\n",
    "\n",
    "# 2. Draw bounding boxes and detection scores on those images\n",
    "def draw_boxes_on_image(image, detection_boxes, detection_scores, threshold=0.5):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    img_width, img_height = image.size\n",
    "    for box, score in zip(detection_boxes, detection_scores):\n",
    "        if score >= threshold:\n",
    "            y_min, x_min, y_max, x_max = box\n",
    "            # Convert normalized coordinates to pixel values\n",
    "            left, right, top, bottom = x_min * img_width, x_max * img_width, y_min * img_height, y_max * img_height\n",
    "            draw.rectangle([(left, top), (right, bottom)], outline=\"green\", width=2)\n",
    "            label = f\"Score: {score:.2f}\"\n",
    "            # Using PIL might not have fonts by default, so adjusting the text size instead\n",
    "            draw.text((left, top - 12), label, fill=\"green\")\n",
    "    return image\n",
    "\n",
    "for image_path in low_iou_images:\n",
    "    image = Image.open(image_path)\n",
    "    detections = detect_objects(detection_model, image)\n",
    "    detection_boxes = detections['detection_boxes'][0].numpy()\n",
    "    detection_scores = detections['detection_scores'][0].numpy()\n",
    "    image = draw_boxes_on_image(image, detection_boxes, detection_scores)\n",
    "    image.save(os.path.join(NEW_DIR, os.path.basename(image_path)))\n",
    "\n",
    "# 3. Visualize all these images in a single matplotlib frame using subplots\n",
    "num_images = len(low_iou_images)\n",
    "cols = 5  # Number of columns in the subplot grid (you can adjust this)\n",
    "rows = int(num_images / cols) + 1  # Calculate the number of rows required\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "fig.tight_layout()\n",
    "\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    if idx < num_images:\n",
    "        image_path = os.path.join(NEW_DIR, os.path.basename(low_iou_images[idx]))\n",
    "        image = Image.open(image_path)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "    else:\n",
    "        ax.axis(\"off\")  # Turn off any unused subplots\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_od",
   "language": "python",
   "name": "tf_od"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
